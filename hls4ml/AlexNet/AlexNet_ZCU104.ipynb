{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fead394-727d-4f82-a2bf-929d76eea825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['LD_PRELOAD'] = '/lib/x86_64-linux-gnu/libudev.so.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0450ff7-4fcc-4759-9466-e4486afa5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "cifar10_classes=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72849e20-3899-455a-8b6b-8d530c708732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (QConv2D)             (None, 28, 28, 5)         380       \n",
      "                                                                 \n",
      " relu1 (QActivation)         (None, 28, 28, 5)         0         \n",
      "                                                                 \n",
      " pool1 (MaxPooling2D)        (None, 13, 13, 5)         0         \n",
      "                                                                 \n",
      " conv2 (QConv2D)             (None, 9, 9, 13)          1638      \n",
      "                                                                 \n",
      " relu2 (QActivation)         (None, 9, 9, 13)          0         \n",
      "                                                                 \n",
      " pool2 (MaxPooling2D)        (None, 4, 4, 13)          0         \n",
      "                                                                 \n",
      " conv3 (QConv2D)             (None, 4, 4, 20)          2360      \n",
      "                                                                 \n",
      " relu3 (QActivation)         (None, 4, 4, 20)          0         \n",
      "                                                                 \n",
      " conv4 (QConv2D)             (None, 4, 4, 20)          3620      \n",
      "                                                                 \n",
      " relu4 (QActivation)         (None, 4, 4, 20)          0         \n",
      "                                                                 \n",
      " conv5 (QConv2D)             (None, 4, 4, 13)          2353      \n",
      "                                                                 \n",
      " relu5 (QActivation)         (None, 4, 4, 13)          0         \n",
      "                                                                 \n",
      " pool5 (MaxPooling2D)        (None, 1, 1, 13)          0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 13)                0         \n",
      "                                                                 \n",
      " fc1 (QDense)                (None, 214)               2996      \n",
      "                                                                 \n",
      " relu6 (QActivation)         (None, 214)               0         \n",
      "                                                                 \n",
      " fc2 (QDense)                (None, 16)                3440      \n",
      "                                                                 \n",
      " relu7 (QActivation)         (None, 16)                0         \n",
      "                                                                 \n",
      " output (QDense)             (None, 10)                170       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,957\n",
      "Trainable params: 16,957\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('AlexNet_Weights/AlexNet_Normal_Model.h5')\n",
    "model = load_model('AlexNet_Weights/AlexNet_check_best_model.h5', custom_objects=co)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a61fbf54-7100-4e43-abcf-09ddcf187604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_5, layer type: InputLayer, input shapes: [[None, 32, 32, 3]], output shape: [None, 32, 32, 3]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 32, 32, 3]], output shape: [None, 28, 28, 5]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 28, 28, 5]], output shape: [None, 28, 28, 5]\n",
      "Layer name: pool1, layer type: MaxPooling2D, input shapes: [[None, 28, 28, 5]], output shape: [None, 13, 13, 5]\n",
      "Layer name: conv2, layer type: QConv2D, input shapes: [[None, 13, 13, 5]], output shape: [None, 9, 9, 13]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 9, 9, 13]], output shape: [None, 9, 9, 13]\n",
      "Layer name: pool2, layer type: MaxPooling2D, input shapes: [[None, 9, 9, 13]], output shape: [None, 4, 4, 13]\n",
      "Layer name: conv3, layer type: QConv2D, input shapes: [[None, 4, 4, 13]], output shape: [None, 4, 4, 20]\n",
      "Layer name: relu3, layer type: Activation, input shapes: [[None, 4, 4, 20]], output shape: [None, 4, 4, 20]\n",
      "Layer name: conv4, layer type: QConv2D, input shapes: [[None, 4, 4, 20]], output shape: [None, 4, 4, 20]\n",
      "Layer name: relu4, layer type: Activation, input shapes: [[None, 4, 4, 20]], output shape: [None, 4, 4, 20]\n",
      "Layer name: conv5, layer type: QConv2D, input shapes: [[None, 4, 4, 20]], output shape: [None, 4, 4, 13]\n",
      "Layer name: relu5, layer type: Activation, input shapes: [[None, 4, 4, 13]], output shape: [None, 4, 4, 13]\n",
      "Layer name: pool5, layer type: MaxPooling2D, input shapes: [[None, 4, 4, 13]], output shape: [None, 1, 1, 13]\n",
      "Layer name: flatten_4, layer type: Reshape, input shapes: [[None, 1, 1, 13]], output shape: [None, 13]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 13]], output shape: [None, 214]\n",
      "Layer name: relu6, layer type: Activation, input shapes: [[None, 214]], output shape: [None, 214]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 214]], output shape: [None, 16]\n",
      "Layer name: relu7, layer type: Activation, input shapes: [[None, 16]], output shape: [None, 16]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 16]], output shape: [None, 10]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "-----------------------------------\n",
      "Model\n",
      "  Precision:         ap_fixed<16,6>\n",
      "  ReuseFactor:       1\n",
      "  Strategy:          Latency\n",
      "  BramFactor:        1000000000\n",
      "  TraceOutput:       False\n",
      "LayerName\n",
      "  input_5\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  conv1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<16,7>\n",
      "      bias:          fixed<16,7>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  conv1_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  relu1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  pool1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  conv2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<16,7>\n",
      "      bias:          fixed<16,7>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  conv2_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  relu2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  pool2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  conv3\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<16,7>\n",
      "      bias:          fixed<16,7>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  conv3_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  relu3\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  conv4\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<16,7>\n",
      "      bias:          fixed<16,7>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  conv4_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  relu4\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  conv5\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<16,7>\n",
      "      bias:          fixed<16,7>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  conv5_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  relu5\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  pool5\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  flatten_4\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  fc1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<16,7>\n",
      "      bias:          fixed<16,7>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  fc1_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  relu6\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  fc2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<16,7>\n",
      "      bias:          fixed<16,7>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  fc2_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  relu7\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  output\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<16,7>\n",
      "      bias:          fixed<16,7>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  output_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Latency\n",
      "    ReuseFactor:     1\n",
      "  softmax\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "    Strategy:        Stable\n",
      "    ReuseFactor:     1\n",
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_5, layer type: InputLayer, input shapes: [[None, 32, 32, 3]], output shape: [None, 32, 32, 3]\n",
      "Layer name: conv1, layer type: QConv2D, input shapes: [[None, 32, 32, 3]], output shape: [None, 28, 28, 5]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 28, 28, 5]], output shape: [None, 28, 28, 5]\n",
      "Layer name: pool1, layer type: MaxPooling2D, input shapes: [[None, 28, 28, 5]], output shape: [None, 13, 13, 5]\n",
      "Layer name: conv2, layer type: QConv2D, input shapes: [[None, 13, 13, 5]], output shape: [None, 9, 9, 13]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 9, 9, 13]], output shape: [None, 9, 9, 13]\n",
      "Layer name: pool2, layer type: MaxPooling2D, input shapes: [[None, 9, 9, 13]], output shape: [None, 4, 4, 13]\n",
      "Layer name: conv3, layer type: QConv2D, input shapes: [[None, 4, 4, 13]], output shape: [None, 4, 4, 20]\n",
      "Layer name: relu3, layer type: Activation, input shapes: [[None, 4, 4, 20]], output shape: [None, 4, 4, 20]\n",
      "Layer name: conv4, layer type: QConv2D, input shapes: [[None, 4, 4, 20]], output shape: [None, 4, 4, 20]\n",
      "Layer name: relu4, layer type: Activation, input shapes: [[None, 4, 4, 20]], output shape: [None, 4, 4, 20]\n",
      "Layer name: conv5, layer type: QConv2D, input shapes: [[None, 4, 4, 20]], output shape: [None, 4, 4, 13]\n",
      "Layer name: relu5, layer type: Activation, input shapes: [[None, 4, 4, 13]], output shape: [None, 4, 4, 13]\n",
      "Layer name: pool5, layer type: MaxPooling2D, input shapes: [[None, 4, 4, 13]], output shape: [None, 1, 1, 13]\n",
      "Layer name: flatten_4, layer type: Reshape, input shapes: [[None, 1, 1, 13]], output shape: [None, 13]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 13]], output shape: [None, 214]\n",
      "Layer name: relu6, layer type: Activation, input shapes: [[None, 214]], output shape: [None, 214]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 214]], output shape: [None, 16]\n",
      "Layer name: relu7, layer type: Activation, input shapes: [[None, 16]], output shape: [None, 16]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 16]], output shape: [None, 10]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Creating HLS model\n",
      "WARNING: Cannot use \"Latency\" model strategy for conv1 layer. Switching to \"Resource\" strategy.\n",
      "WARNING: You set a Part that does not correspond to the Board you specified. The correct Part is now set.\n",
      "Writing HLS project\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "config['Model']['Precision'] = 'ap_fixed<16,6>'\n",
    "config['Model']['ReuseFactor'] = 1\n",
    "\n",
    "for Layer in config['LayerName'].keys():\n",
    "    config['LayerName'][Layer]['Strategy'] = 'Latency'\n",
    "    config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "    #config['LayerName'][Layer]['Precision'] = 'ap_fixed<8,4>'\n",
    "\n",
    "config['LayerName']['softmax']['Strategy'] = 'Stable'\n",
    "'''\n",
    "for layer in ['conv1', 'conv2'] :\n",
    "    config['LayerName'][layer]['Precision'] = 'ap_fixed<8,4>'\n",
    "'''\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "cfg = hls4ml.converters.create_config(backend='VivadoAccelerator')\n",
    "cfg['IOType'] = 'io_stream'\n",
    "cfg['HLSConfig'] = config\n",
    "cfg['KerasModel'] = model\n",
    "cfg['OutputDir'] = 'AlexNet_ZCU104'\n",
    "cfg['Board'] = 'zcu104'\n",
    "\n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0249eed-82db-47c0-92e9-3dd75c46aa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)\n",
      "  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019\n",
      "  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019\n",
      "    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.\n",
      "\n",
      "source /opt/Xilinx/Vivado/2019.2/scripts/vivado_hls/hls.tcl -notrace\n",
      "INFO: Applying HLS Y2K22 patch v1.2 for IP revision\n",
      "INFO: [HLS 200-10] Running '/opt/Xilinx/Vivado/2019.2/bin/unwrapped/lnx64.o/vivado_hls'\n",
      "INFO: [HLS 200-10] For user 'jovyan' on host '6307b0c947c6' (Linux_x86_64 version 4.15.0-212-generic) on Fri Jul 07 04:55:02 UTC 2023\n",
      "INFO: [HLS 200-10] In directory '/home/jovyan/Internship_Waseda/hls4ml/AlexNet/AlexNet_ZCU104'\n",
      "Sourcing Tcl script 'build_prj.tcl'\n",
      "INFO: [HLS 200-10] Opening project '/home/jovyan/Internship_Waseda/hls4ml/AlexNet/AlexNet_ZCU104/myproject_prj'.\n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject_axi.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'myproject_test.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project\n",
      "INFO: [HLS 200-10] Opening solution '/home/jovyan/Internship_Waseda/hls4ml/AlexNet/AlexNet_ZCU104/myproject_prj/solution1'.\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with a period of 5ns.\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with an uncertainty of 0.625ns.\n",
      "INFO: [HLS 200-10] Setting target device to 'xc7z020-clg400-1'\n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set into 80.\n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set into 80.\n",
      "***** C SIMULATION *****\n",
      "INFO: [SIM 211-2] *************** CSIM start ***************\n",
      "INFO: [SIM 211-4] CSIM will launch GCC as the compiler.\n",
      "   Compiling ../../../../myproject_test.cpp in debug mode\n",
      "   Compiling ../../../../firmware/myproject.cpp in debug mode\n",
      "   Compiling ../../../../firmware/myproject_axi.cpp in debug mode\n",
      "   Generating csim.exe\n",
      "INFO: Unable to open input/predictions file, using default input.\n",
      "csim.exe: ../../../../firmware/nnet_utils/nnet_pooling_stream.h:234: void nnet::pooling2d_buffer_cl(hls::stream<srcType>&, hls::stream<dstType>&) [with data_T = nnet::array<ap_ufixed<6, 0, (ap_q_mode)4u, (ap_o_mode)0u>, 5u>; res_T = nnet::array<ap_fixed<16, 6>, 5u>; CONFIG_T = config5]: Assertion `CONFIG_T::pool_height == CONFIG_T::stride_height && CONFIG_T::pool_width == CONFIG_T::stride_width' failed.\n",
      "@E Simulation failed.\n",
      "ERROR: [SIM 211-100] CSim failed with errors.\n",
      "INFO: [SIM 211-3] *************** CSIM finish ***************\n",
      "4\n",
      "    while executing\n",
      "\"source build_prj.tcl\"\n",
      "    (\"uplevel\" body line 1)\n",
      "    invoked from within\n",
      "\"uplevel \\#0 [list source $arg] \"\n",
      "\n",
      "INFO: [Common 17-206] Exiting vivado_hls at Fri Jul  7 04:55:13 2023...\n",
      "CSynthesis report not found.\n",
      "Vivado synthesis report not found.\n",
      "Cosim report not found.\n",
      "Timing report not found.\n",
      "\n",
      "****** Vivado v2019.2 (64-bit)\n",
      "  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019\n",
      "  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019\n",
      "    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.\n",
      "\n",
      "source design.tcl\n",
      "# set tcldir [file dirname [info script]]\n",
      "# source [file join $tcldir project.tcl]\n",
      "## variable project_name\n",
      "## set project_name \"myproject\"\n",
      "## variable backend\n",
      "## set backend \"vivadoaccelerator\"\n",
      "## variable part\n",
      "## set part \"xc7z020clg400-1\"\n",
      "## variable clock_period\n",
      "## set clock_period 5\n",
      "## variable clock_uncertainty\n",
      "## set clock_uncertainty 12.5%\n",
      "## set bit_width_hls_output 32\n",
      "## set bit_width_hls_input 32\n",
      "# create_project project_1 ${project_name}_vivado_accelerator -part xc7z020clg400-1 -force\n",
      "# set_property board_part tul.com.tw:pynq-z2:part0:1.0 [current_project]\n",
      "# set_property  ip_repo_paths  ${project_name}_prj [current_project]\n",
      "# update_ip_catalog\n",
      "INFO: [IP_Flow 19-234] Refreshing IP repositories\n",
      "INFO: [IP_Flow 19-1700] Loaded user IP repository '/home/jovyan/Internship_Waseda/hls4ml/AlexNet/AlexNet_ZCU104/myproject_prj'.\n",
      "INFO: [IP_Flow 19-2313] Loaded Vivado IP repository '/opt/Xilinx/Vivado/2019.2/data/ip'.\n",
      "# create_bd_design \"design_1\"\n",
      "Wrote  : </home/jovyan/Internship_Waseda/hls4ml/AlexNet/AlexNet_ZCU104/myproject_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/design_1.bd> \n",
      "# startgroup\n",
      "# create_bd_cell -type ip -vlnv xilinx.com:ip:processing_system7:5.5 processing_system7_0\n",
      "WARNING: [BD 41-176] The physical port 'S_AXI_GP2_rd_socket' specified in the portmap, is not found on the block! \n",
      "WARNING: [BD 41-176] The physical port 'S_AXI_GP2_wr_socket' specified in the portmap, is not found on the block! \n",
      "WARNING: [BD 41-176] The physical port 'S_AXI_GP3_rd_socket' specified in the portmap, is not found on the block! \n",
      "WARNING: [BD 41-176] The physical port 'S_AXI_GP3_wr_socket' specified in the portmap, is not found on the block! \n",
      "# endgroup\n",
      "# apply_bd_automation -rule xilinx.com:bd_rule:processing_system7 -config {make_external \"FIXED_IO, DDR\" apply_board_preset \"1\" Master \"Disable\" Slave \"Disable\" }  [get_bd_cells processing_system7_0]\n",
      "# startgroup\n",
      "# set_property -dict [list CONFIG.PCW_USE_S_AXI_HP0 {1}] [get_bd_cells processing_system7_0]\n",
      "CRITICAL WARNING: [PSU-1]  Parameter : PCW_UIPARAM_DDR_DQS_TO_CLK_DELAY_0 has negative value -0.051 . PS DDR interfaces might fail when entering negative DQS skew values. \n",
      "CRITICAL WARNING: [PSU-2]  Parameter : PCW_UIPARAM_DDR_DQS_TO_CLK_DELAY_1 has negative value -0.006 . PS DDR interfaces might fail when entering negative DQS skew values. \n",
      "CRITICAL WARNING: [PSU-3]  Parameter : PCW_UIPARAM_DDR_DQS_TO_CLK_DELAY_2 has negative value -0.009 . PS DDR interfaces might fail when entering negative DQS skew values. \n",
      "CRITICAL WARNING: [PSU-4]  Parameter : PCW_UIPARAM_DDR_DQS_TO_CLK_DELAY_3 has negative value -0.033 . PS DDR interfaces might fail when entering negative DQS skew values. \n",
      "WARNING: [BD 41-176] The physical port 'S_AXI_GP2_rd_socket' specified in the portmap, is not found on the block! \n",
      "WARNING: [BD 41-176] The physical port 'S_AXI_GP2_wr_socket' specified in the portmap, is not found on the block! \n",
      "WARNING: [BD 41-176] The physical port 'S_AXI_GP3_rd_socket' specified in the portmap, is not found on the block! \n",
      "WARNING: [BD 41-176] The physical port 'S_AXI_GP3_wr_socket' specified in the portmap, is not found on the block! \n",
      "# endgroup\n",
      "# startgroup\n",
      "# create_bd_cell -type ip -vlnv xilinx.com:ip:axi_dma:7.1 axi_dma_0\n",
      "# endgroup\n",
      "# set_property -dict [list CONFIG.c_s_axis_s2mm_tdata_width.VALUE_SRC USER] [get_bd_cells axi_dma_0]\n",
      "# set_property -dict [list CONFIG.c_include_sg {0} CONFIG.c_sg_length_width {26} CONFIG.c_sg_include_stscntrl_strm {0} CONFIG.c_m_axi_mm2s_data_width ${bit_width_hls_input} CONFIG.c_m_axis_mm2s_tdata_width ${bit_width_hls_input} CONFIG.c_mm2s_burst_size {256} CONFIG.c_s_axis_s2mm_tdata_width ${bit_width_hls_output} CONFIG.c_s_axis_s2mm_data_width ${bit_width_hls_output} CONFIG.c_s2mm_burst_size {256}] [get_bd_cells axi_dma_0]\n",
      "CRITICAL WARNING: [BD 41-1276] Cannot set the parameter c_s_axis_s2mm_data_width on /axi_dma_0. Parameter does not exist\n",
      "# startgroup\n",
      "# apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {Auto} Clk_slave {Auto} Clk_xbar {Auto} Master {/processing_system7_0/M_AXI_GP0} Slave {/axi_dma_0/S_AXI_LITE} ddr_seg {Auto} intc_ip {New AXI Interconnect} master_apm {0}}  [get_bd_intf_pins axi_dma_0/S_AXI_LITE]\n",
      "INFO: [Ipptcl 7-1463] No Compatible Board Interface found. Board Tab not created in customize GUI\n",
      "Slave segment </axi_dma_0/S_AXI_LITE/Reg> is being mapped into address space </processing_system7_0/Data> at <0x4040_0000 [ 64K ]>\n",
      "# apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {Auto} Clk_slave {Auto} Clk_xbar {Auto} Master {/axi_dma_0/M_AXI_MM2S} Slave {/processing_system7_0/S_AXI_HP0} ddr_seg {Auto} intc_ip {New AXI Interconnect} master_apm {0}}  [get_bd_intf_pins processing_system7_0/S_AXI_HP0]\n",
      "Slave segment </processing_system7_0/S_AXI_HP0/HP0_DDR_LOWOCM> is being mapped into address space </axi_dma_0/Data_MM2S> at <0x0000_0000 [ 512M ]>\n",
      "# endgroup\n",
      "# apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {Auto} Clk_slave {/processing_system7_0/FCLK_CLK0 (100 MHz)} Clk_xbar {/processing_system7_0/FCLK_CLK0 (100 MHz)} Master {/axi_dma_0/M_AXI_S2MM} Slave {/processing_system7_0/S_AXI_HP0} ddr_seg {Auto} intc_ip {/axi_mem_intercon} master_apm {0}}  [get_bd_intf_pins axi_dma_0/M_AXI_S2MM]\n",
      "Slave segment </processing_system7_0/S_AXI_HP0/HP0_DDR_LOWOCM> is being mapped into address space </axi_dma_0/Data_S2MM> at <0x0000_0000 [ 512M ]>\n",
      "# startgroup\n",
      "# create_bd_cell -type ip -vlnv xilinx.com:hls:${project_name}_axi:1.0 ${project_name}_axi_0\n",
      "ERROR: [Common 17-39] 'create_bd_cell' failed due to earlier errors.\n",
      "\n",
      "    while executing\n",
      "\"create_bd_cell -type ip -vlnv xilinx.com:hls:${project_name}_axi:1.0 ${project_name}_axi_0\"\n",
      "    (file \"design.tcl\" line 39)\n",
      "INFO: [Common 17-17] undo 'startgroup'\n",
      "INFO: [Common 17-206] Exiting Vivado at Fri Jul  7 04:55:29 2023...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [BD 5-390] IP definition not found for VLNV: xilinx.com:hls:myproject_axi:1.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSynthesis report not found.\n",
      "Vivado synthesis report not found.\n",
      "Cosim report not found.\n",
      "Timing report not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CSimResults': []}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hls_model.build(csim=True, export=True, bitfile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590f3a6-7485-49a8-a490-3c87499dffb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
