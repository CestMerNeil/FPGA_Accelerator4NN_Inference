{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 04:05:52.977147: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['LD_PRELOAD'] = '/lib/x86_64-linux-gnu/libudev.so.1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the jet tagging dataset from Open ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "#x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "np.save('X_test.npy', x_test)\n",
    "np.save('y_test.npy', y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks\n",
    "from tensorflow.keras.layers import Activation\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras import QConv2D\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "from tensorflow.keras.layers import Flatten, MaxPooling2D, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 04:05:55.713287: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer LecunUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (QConv2D)             (None, 24, 24, 16)        416       \n",
      "                                                                 \n",
      " relu1 (QActivation)         (None, 24, 24, 16)        0         \n",
      "                                                                 \n",
      " pool1 (MaxPooling2D)        (None, 12, 12, 16)        0         \n",
      "                                                                 \n",
      " conv2 (QConv2D)             (None, 8, 8, 4)           1604      \n",
      "                                                                 \n",
      " relu2 (QActivation)         (None, 8, 8, 4)           0         \n",
      "                                                                 \n",
      " pool2 (MaxPooling2D)        (None, 4, 4, 4)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " fc1 (QDense)                (None, 30)                1950      \n",
      "                                                                 \n",
      " fc2 (QDense)                (None, 10)                310       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,280\n",
      "Trainable params: 4,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    Input(shape=(28, 28, 1))\n",
    ")\n",
    "\n",
    "# Conv 1\n",
    "model.add(\n",
    "    QConv2D(\n",
    "        16,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(1, 1),\n",
    "        padding='valid',\n",
    "        name='conv1',\n",
    "        kernel_quantizer=quantized_bits(8, 4, alpha=1),\n",
    "        bias_quantizer=quantized_bits(8, 4, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001)\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    QActivation(activation=quantized_relu(4), name='relu1')\n",
    ")\n",
    "model.add(\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid', name='pool1')\n",
    ")\n",
    "\n",
    "# Conv 2\n",
    "model.add(\n",
    "    QConv2D(\n",
    "        4,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(1, 1),\n",
    "        padding='valid',\n",
    "        name='conv2',\n",
    "        kernel_quantizer=quantized_bits(8, 4, alpha=1),\n",
    "        bias_quantizer=quantized_bits(8, 4, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001)\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    QActivation(activation=quantized_relu(4), name='relu2')\n",
    ")\n",
    "model.add(\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid', name='pool2')\n",
    ")\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# FC 1\n",
    "model.add(\n",
    "    QDense(\n",
    "        30,\n",
    "        name='fc1',\n",
    "        kernel_quantizer=quantized_bits(8, 4, alpha=1),\n",
    "        bias_quantizer=quantized_bits(8, 4, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001)\n",
    "    )\n",
    ")\n",
    "\n",
    "# FC 2\n",
    "model.add(\n",
    "    QDense(\n",
    "        10,\n",
    "        name='fc2',\n",
    "        kernel_quantizer=quantized_bits(8, 4, alpha=1),\n",
    "        bias_quantizer=quantized_bits(8, 4, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001)\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(Activation(activation='softmax', name='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "pruning_params = {\"pruning_schedule\": pruning_schedule.ConstantSparsity(0.75, begin_step=2000, frequency=100)}\n",
    "model = prune.prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "59/59 [==============================] - 9s 129ms/step - loss: 0.4486 - accuracy: 0.9059 - val_loss: 0.4232 - val_accuracy: 0.9105\n",
      "Epoch 2/30\n",
      "59/59 [==============================] - 7s 123ms/step - loss: 0.4276 - accuracy: 0.9080 - val_loss: 0.4005 - val_accuracy: 0.9170\n",
      "Epoch 3/30\n",
      "59/59 [==============================] - 7s 123ms/step - loss: 0.4089 - accuracy: 0.9116 - val_loss: 0.3809 - val_accuracy: 0.9191\n",
      "Epoch 4/30\n",
      "59/59 [==============================] - 7s 125ms/step - loss: 0.3909 - accuracy: 0.9153 - val_loss: 0.3790 - val_accuracy: 0.9159\n",
      "Epoch 5/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.3812 - accuracy: 0.9169 - val_loss: 0.3682 - val_accuracy: 0.9202\n",
      "Epoch 6/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.3698 - accuracy: 0.9196 - val_loss: 0.3465 - val_accuracy: 0.9263\n",
      "Epoch 7/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.3560 - accuracy: 0.9225 - val_loss: 0.3355 - val_accuracy: 0.9253\n",
      "Epoch 8/30\n",
      "59/59 [==============================] - 7s 123ms/step - loss: 0.3459 - accuracy: 0.9251 - val_loss: 0.3266 - val_accuracy: 0.9292\n",
      "Epoch 9/30\n",
      "59/59 [==============================] - 7s 123ms/step - loss: 0.3376 - accuracy: 0.9262 - val_loss: 0.3290 - val_accuracy: 0.9267\n",
      "Epoch 10/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.3297 - accuracy: 0.9282 - val_loss: 0.3167 - val_accuracy: 0.9293\n",
      "Epoch 11/30\n",
      "59/59 [==============================] - 7s 123ms/step - loss: 0.3214 - accuracy: 0.9299 - val_loss: 0.3084 - val_accuracy: 0.9310\n",
      "Epoch 12/30\n",
      "59/59 [==============================] - 7s 123ms/step - loss: 0.3133 - accuracy: 0.9315 - val_loss: 0.2979 - val_accuracy: 0.9345\n",
      "Epoch 13/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.3071 - accuracy: 0.9328 - val_loss: 0.2897 - val_accuracy: 0.9386\n",
      "Epoch 14/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.2995 - accuracy: 0.9350 - val_loss: 0.2878 - val_accuracy: 0.9355\n",
      "Epoch 15/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.2941 - accuracy: 0.9360 - val_loss: 0.2846 - val_accuracy: 0.9378\n",
      "Epoch 16/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.2904 - accuracy: 0.9371 - val_loss: 0.2731 - val_accuracy: 0.9395\n",
      "Epoch 17/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.2860 - accuracy: 0.9383 - val_loss: 0.2717 - val_accuracy: 0.9400\n",
      "Epoch 18/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.2811 - accuracy: 0.9391 - val_loss: 0.2689 - val_accuracy: 0.9398\n",
      "Epoch 19/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.2755 - accuracy: 0.9413 - val_loss: 0.2579 - val_accuracy: 0.9458\n",
      "Epoch 20/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.2692 - accuracy: 0.9423 - val_loss: 0.2547 - val_accuracy: 0.9456\n",
      "Epoch 21/30\n",
      "59/59 [==============================] - 7s 123ms/step - loss: 0.2652 - accuracy: 0.9430 - val_loss: 0.2582 - val_accuracy: 0.9424\n",
      "Epoch 22/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.2627 - accuracy: 0.9435 - val_loss: 0.2523 - val_accuracy: 0.9438\n",
      "Epoch 23/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.2598 - accuracy: 0.9436 - val_loss: 0.2424 - val_accuracy: 0.9450\n",
      "Epoch 24/30\n",
      "59/59 [==============================] - 7s 125ms/step - loss: 0.2529 - accuracy: 0.9452 - val_loss: 0.2354 - val_accuracy: 0.9478\n",
      "Epoch 25/30\n",
      "59/59 [==============================] - 7s 125ms/step - loss: 0.2488 - accuracy: 0.9464 - val_loss: 0.2346 - val_accuracy: 0.9483\n",
      "Epoch 26/30\n",
      "59/59 [==============================] - 7s 125ms/step - loss: 0.2453 - accuracy: 0.9473 - val_loss: 0.2382 - val_accuracy: 0.9481\n",
      "Epoch 27/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.2438 - accuracy: 0.9467 - val_loss: 0.2271 - val_accuracy: 0.9512\n",
      "Epoch 28/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.2415 - accuracy: 0.9472 - val_loss: 0.2269 - val_accuracy: 0.9507\n",
      "Epoch 29/30\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.2365 - accuracy: 0.9491 - val_loss: 0.2280 - val_accuracy: 0.9507\n",
      "Epoch 30/30\n",
      "59/59 [==============================] - 7s 125ms/step - loss: 0.2342 - accuracy: 0.9502 - val_loss: 0.2230 - val_accuracy: 0.9529\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "if train:\n",
    "    adam = Adam(learning_rate=0.0001)\n",
    "    model.compile(\n",
    "        optimizer=adam, \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=1024,\n",
    "        epochs=30,\n",
    "        validation_data=(x_test, y_test),\n",
    "        shuffle=True,\n",
    "    )\n",
    "    model = strip_pruning(model)\n",
    "    model.save('LeNet_Prune_Model.h5')\n",
    "else:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    model = load_model('model_3/KERAS_check_best_model.h5', custom_objects=co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "606c0b78d9e04732b14ee9e45c5bfff9d12dee0e0feabd86b8db33116293c133"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
